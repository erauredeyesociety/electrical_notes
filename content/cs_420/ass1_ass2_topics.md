# Operating Systems Theory - Complete Study Guide
*Comprehensive coverage for Assignments 1 & 2*

---

## 1. Functions of an Operating System

### What is an Operating System?
An operating system is an intermediary program that sits between users and computer hardware, creating an environment where other programs can do useful work. Like a government, it doesn't do productive work itself but provides the framework for others to operate efficiently.

### Primary Goals
1. **Execute user programs** - Make solving user problems easier
2. **Convenience** - Make the computer system easy to use
3. **Efficiency** - Use hardware resources optimally
4. **Reliability & Safety** - Prevent errors and improper computer use

### Core Roles

**Resource Allocator**
The OS manages all system resources (CPU time, memory space, file storage, I/O devices) and mediates between conflicting requests to ensure efficient and fair usage.

**Control Program**
Controls program execution to prevent errors and improper computer use, maintaining system integrity and security.

### Key Operating System Services

| Service | Description |
|---------|-------------|
| **User Interface** | Provides CLI, GUI, or Batch interfaces for user interaction |
| **Program Execution** | Loads programs into memory, executes them, handles termination (normal or error) |
| **I/O Operations** | Manages I/O device operations, file access, and device communication |
| **File-System Manipulation** | Enables reading, writing, creating, deleting files; managing directories and permissions |
| **Communications (IPC)** | Allows processes to exchange information via shared memory or message passing |
| **Error Detection** | Constantly monitors for hardware/software errors and takes corrective action |
| **Resource Allocation** | Distributes CPU cycles, memory, storage, and I/O among concurrent processes |
| **Protection & Security** | Controls resource access and defends against unauthorized external access |

### System Components

**Four Components of a Computer System:**
1. **Hardware** - CPU, memory, I/O devices (provides basic computing resources)
2. **Operating System** - Controls and coordinates hardware use among applications
3. **Application Programs** - Define how system resources solve user problems
4. **Users** - People, machines, or other computers using the system

**What runs all the time?** The **kernel** is the one program running continuously on the computer. Everything else is either a system program (shipped with OS) or an application program.

---

## 2. Interrupt Handling

### What are Interrupts?

The operating system is **interrupt-driven** - it responds to events rather than running continuously. Interrupts signal that something requires the OS's attention.

**Types of Interrupts:**
- **Hardware Interrupt** - Generated by hardware devices (e.g., I/O device completing an operation)
- **Software Interrupt (Trap/Exception)** - Caused by software errors (division by zero) or user requests (system calls)

### Interrupt Handling Sequence

1. **Interrupt Trigger**
   - An I/O device controller signals completion by causing a hardware interrupt
   - OR a trap/exception occurs from an error or system call

2. **Control Transfer**
   - CPU stops current execution
   - Control transfers to the **interrupt service routine** (ISR)
   - Transfer uses the **interrupt vector** - a table containing memory addresses of all service routines

3. **State Preservation**
   - The address of the interrupted instruction is saved
   - CPU registers and process state are saved to allow resumption later

4. **Kernel Mode Entry**
   - If the interrupt is a system call requesting OS services, the **mode bit** changes to **kernel mode** (supervisor mode)
   - This allows execution of privileged instructions that user programs cannot execute

5. **Service Routine Execution**
   - The CPU executes the appropriate service routine based on the interrupt type
   - The routine handles the event (service I/O, respond to system call, handle error)

6. **Return to User Mode**
   - After handling the interrupt, the mode bit changes back to **user mode**
   - Execution resumes from the saved instruction address
   - The system returns control to the interrupted process

### CPU Mode Switching

**User Mode** - Limited instruction set; cannot execute privileged operations
**Kernel Mode** - Full access to all hardware and memory; can execute any instruction

The mode bit prevents user programs from interfering with OS operations or other users' programs.

### Timer Interrupts

A **timer** (set by privileged instruction) can interrupt the computer after a specified period, ensuring the OS regains control periodically. This prevents any single process from monopolizing the CPU and enables time-sharing systems.

---

## 3. Bootstrapping and System Calls

### Bootstrap Process

**What is a Bootstrap Program?**
A small piece of code that initializes the system when the computer is powered on or rebooted. It locates and loads the operating system kernel into memory.

**Where is it stored?**
In ROM (Read-Only Memory), EPROM (Erasable Programmable ROM), or firmware. This non-volatile storage ensures the bootstrap code is always available at startup.

### System Calls

**What are System Calls?**
System calls provide the **interface to OS services** - they are the mechanism by which user programs request services from the operating system kernel.

**API, System-Call Interface, and OS Relationship:**
- **API (Application Programming Interface)** - High-level functions programmers use (e.g., `open()`, `read()`)
- **System-Call Interface** - Intercepts API calls and invokes appropriate system calls in the OS
- **Operating System** - Executes the requested service in kernel mode

**Three Methods to Pass Parameters:**
1. **Registers** - Pass parameters directly in CPU registers (simplest, but limited)
2. **Memory Block/Table** - Store parameters in memory, pass address in register (used when many parameters)
3. **Stack** - Push parameters onto the program stack, OS pops them off (flexible)

---

## 4. Process Management

### Process Fundamentals

**What is a Process?**
A **process** is a program in execution - the active unit of work in a system. A program is merely a passive entity (executable file), while a process is active with a program counter and resources.

### Process Memory Layout

A process's memory is divided into sections:
- **Text Section** - Executable code
- **Data Section** - Global variables
- **Heap Section** - Dynamically allocated memory during runtime (grows upward)
- **Stack Section** - Temporary data (function parameters, return addresses, local variables)

**Question: What area contains dynamically allocated data?**
The **heap section** contains data allocated during runtime via functions like `malloc()`.

### Process States

A process changes state as it executes:

| State | Description |
|-------|-------------|
| **New** | Process is being created |
| **Running** | Instructions are being executed on CPU |
| **Waiting** | Process is waiting for an event (I/O completion, signal) |
| **Ready** | Process is waiting to be assigned to a CPU |
| **Terminated** | Process has finished execution |

**State Transitions:**
- Running → Waiting: Process makes I/O request
- Running → Ready: Interrupt occurs (time slice expires)
- Waiting → Ready: I/O or event completes
- Ready → Running: Scheduler selects this process
- Running → Terminated: Process completes

**When is a process forced off the CPU?**
- I/O request (voluntary)
- Time slice expiration (involuntary preemption)
- Interrupt occurrence (involuntary)
- Calling `fork()` may trigger scheduling (depends on implementation)

### Process Control Block (PCB)

The **PCB** is the data structure containing all information associated with a process:
- Process state
- Program counter (next instruction address)
- CPU registers (contents of all process-centric registers)
- CPU scheduling information (priority, queue pointers)
- Memory management information (page tables, segment tables)
- Accounting information (CPU used, time limits)
- I/O status information (allocated devices, open files)

### Process Creation

**Parent-Child Relationships:**
Parent processes create children processes, forming a **process tree**. The root is the init process (PID 1 on UNIX systems).

**UNIX Process Creation:**
- `fork()` - System call that creates a new process (child) by duplicating the parent
  - Child receives a copy of the parent's address space
  - Returns 0 to child, child's PID to parent
- `exec()` - Replaces the child process's memory space with a new program
  - Overlays the current process image with a new program

**What happens if you call `exec()` before `fork()`?**
If you call `exec()` before `fork()`, the current process is replaced by the new program, and the `fork()` call never executes. You cannot create a child process because the code containing `fork()` has been overwritten.

**Why does Google Chrome use multiple processes?**
Chrome uses separate processes for different tabs and plugins to provide isolation - if one tab crashes or is compromised, it doesn't affect other tabs. This improves stability, security, and allows better resource management.

### Interprocess Communication (IPC)

Cooperating processes need mechanisms to share information. Two fundamental models:

**1. Shared Memory**
- Processes share a region of memory
- Fast (no kernel intervention after setup)
- Requires synchronization by processes themselves
- Used in Assignment 1 for bounded buffer

**2. Message Passing**
- Processes communicate by sending/receiving messages through the kernel
- Slower (kernel mediation for each message)
- Easier to implement, especially for distributed systems
- No synchronization conflicts

---

## 5. Thread Management

### Thread Fundamentals

**What is a Thread?**
A **thread** is a fundamental unit of CPU utilization, consisting of a thread ID, program counter, register set, and stack. Multiple threads within the same process share code, data, and files.

**Single-threaded vs. Multithreaded Process:**
- Single-threaded: One program counter, one sequence of execution
- Multithreaded: Multiple program counters, multiple concurrent execution sequences

**Why shouldn't a web server be single-threaded?**
A single-threaded web server can only handle one request at a time. While waiting for disk I/O or network operations for one client, it cannot service other clients, resulting in terrible performance and responsiveness.

### Items Shared Among Threads

Threads of the same process share:
- **Code section** (text)
- **Data section** (global variables)
- **Open files** and other OS resources
- Heap memory

Each thread has its own:
- Program counter
- Register set
- Stack (for local variables and function calls)

### Four Major Benefits of Multithreading

**1. Responsiveness**
Allows a program to continue running even if part of it is blocked. Critical for user interfaces - one thread can handle user input while another performs lengthy computation.

**2. Resource Sharing**
Threads automatically share the memory and resources of their parent process. This is simpler and more efficient than implementing shared memory or message passing between separate processes.

**3. Economy**
Thread creation and management is "lightweight" compared to "heavy-weight" process creation. Creating a thread is typically 10-100 times faster than creating a process. Context switching between threads of the same process is also much cheaper than process context switching.

**4. Scalability**
A multithreaded process can take advantage of multiprocessor architectures, with threads running in parallel on different cores. This enables true parallelism on multicore systems.

### Multithreading Models

Threads exist at two levels: **user threads** (managed by user-level libraries) and **kernel threads** (managed by the OS).

| Model | Mapping | Assessment |
|-------|---------|------------|
| **Many-to-One** | Many user threads → 1 kernel thread | Poor concurrency: if one thread blocks, all block. Rarely used today. |
| **One-to-One** | 1 user thread → 1 kernel thread | Good concurrency, commonly used (Windows, Linux). Overhead may limit total thread count. |
| **Many-to-Many** | Many user threads → fewer/equal kernel threads | OS creates optimal number of kernel threads. Flexible but complex. |

**Common Thread Libraries:**
- POSIX Pthreads (UNIX/Linux)
- Windows threads
- Java threads

### Thread Pools

**What is a Thread Pool?**
A set of pre-created threads that wait for work to arrive. Instead of creating a new thread for each task, tasks are submitted to the pool, and an available thread executes them.

**Why use Thread Pools?**
- **Faster service** - No delay for thread creation
- **Limited resources** - Bounds the number of concurrent threads, preventing resource exhaustion
- **Separation of concerns** - Separates task creation from task execution mechanics

---

## 6. CPU Scheduling

### Scheduling Fundamentals

**What is CPU Scheduling?**
The **short-term scheduler** (CPU scheduler) selects which process from the ready queue should execute next on the CPU.

**Dispatcher:**
The dispatcher is the module that gives control of the CPU to the process selected by the scheduler. It performs:
- Context switching
- Switching to user mode
- Jumping to the proper location in the user program

**Dispatch latency** is the time it takes to stop one process and start another.

### When Does Scheduling Occur?

**Nonpreemptive (Cooperative) Scheduling:**
Scheduling decisions occur only when a process:
1. Switches from running to waiting state (I/O request)
2. Terminates

The process voluntarily relinquishes the CPU.

**Preemptive Scheduling:**
Scheduling decisions also occur when a process:
3. Switches from running to ready (interrupt, time slice expires)
4. Switches from waiting to ready (I/O completes, higher priority process arrives)

The OS can forcibly remove a process from the CPU.

### Scheduling Criteria

Goals for scheduling algorithm performance:

| Criterion | Goal | Most Important For |
|-----------|------|-------------------|
| **CPU Utilization** | Maximize | General efficiency |
| **Throughput** | Maximize (processes completed per time unit) | Batch systems |
| **Turnaround Time** | Minimize (submission to completion time) | Batch systems |
| **Waiting Time** | Minimize (time spent in ready queue) | All systems |
| **Response Time** | Minimize (request to first response time) | **Interactive systems** |

**Which is most important for interactive systems?**
**Response time** - users notice delays between requests and responses.

### Scheduling Algorithms

#### 1. First-Come, First-Served (FCFS)

**Description:**
Processes are served in arrival order using a FIFO queue. Nonpreemptive.

**Must be nonpreemptive?** Yes, FCFS is always nonpreemptive.

**Assessment:**
- **Advantage:** Simple to implement
- **Disadvantage:** **Convoy effect** - short processes get stuck behind long processes, causing high average waiting time
- **Disk scheduling disadvantage:** Long waits because head position is ignored

#### 2. Shortest-Job-First (SJF)

**Description:**
Schedules the process with the shortest next CPU burst. Burst length can be predicted using exponential averaging of past bursts.

**Assessment:**
- **Advantage:** **Optimal** - gives minimum average waiting time for a given set of processes
- **Disadvantage:** Difficulty predicting the length of the next CPU burst
- **Problem:** Can cause **starvation** of longer processes

#### 3. Shortest-Remaining-Time-First (SRTF)

**Description:**
Preemptive version of SJF. When a new process arrives with a shorter burst time than the remaining time of the currently running process, the current process is preempted.

**Assessment:**
- Optimal for preemptive scenarios
- High context switch overhead

#### 4. Priority Scheduling

**Description:**
CPU allocated based on priority number (typically smallest integer = highest priority). Can be preemptive or nonpreemptive.

**Preemption Variants:**
- **PR noPREMP (Assignment 2):** Scheduling only on termination; select highest priority ready process
- **PR withPREMP (Assignment 2):** Also schedule when higher priority process arrives, preempting current process

**Assessment:**
- **Advantage:** Important tasks get CPU time
- **Problem:** **Starvation** - low-priority processes may never execute
- **Solution:** **Aging** - gradually increase the priority of processes waiting a long time

**What is Starvation?**
A situation where a process waits indefinitely because other processes continually receive preference.

**How does Aging prevent it?**
By increasing a process's priority as it waits longer, aging eventually makes every waiting process competitive for CPU time.

#### 5. Round Robin (RR)

**Description:**
Each process gets a small **time quantum** (typically 10-100 milliseconds). After the quantum expires, the process is preempted and added to the end of the ready queue (circular FIFO).

**Assignment 2 specifics:** Processes added to queue in arrival order; if quantum expires, go to end of queue.

**Assessment:**
- **Advantage:** Good response time, fair
- **Disadvantage:** High overhead if quantum too small (relative to context switch time)
- If quantum too large, behaves like FCFS

#### 6. Multilevel Feedback Queue

**Description:**
Multiple ready queues with different priorities. Processes can move between queues based on CPU behavior.

**Typical Configuration:**
- High-priority queues use RR with small quanta (interactive jobs)
- Low-priority queues use RR with large quanta or FCFS (CPU-bound jobs)

**Assessment:**
- **Advantage:** Aging easily implemented; adapts to process behavior
- **Disadvantage:** Highly configurable but complex to tune

### Real-Time Scheduling Concepts

**Two Types of Latency:**
1. **Interrupt Latency** - Time from interrupt arrival to service routine start
2. **Dispatch Latency** - Time to stop current process and start new one

Both must be minimized for real-time systems to meet deadlines.

### Concurrency vs. Parallelism

**Concurrency:**
Multiple tasks making progress, typically by multiplexing CPU time (time-sharing). Possible on a single-processor system.

**Parallelism:**
Multiple tasks executing simultaneously, requires multi-core or multi-processor hardware.

**Which term describes multiple tasks making progress on a single processor?**
**Concurrency** - tasks interleave execution through time-slicing.

---

## 7. Concurrency and Synchronization

### The Problem: Race Conditions

**What is a Race Condition?**
Occurs when multiple processes concurrently access and manipulate shared data, and the outcome depends on the particular order of access. This leads to inconsistent or incorrect results.

**Example:** Two processes incrementing a shared counter - the final value depends on the timing of their memory reads and writes.

### Critical Section Problem

**Critical Section (CS):**
A segment of code where a process accesses shared resources that must not be accessed by other processes concurrently.

**Three Requirements for a Solution:**

1. **Mutual Exclusion**
   If process Pi is executing in its critical section, no other process can execute in their critical sections.

2. **Progress**
   If no process is in the CS and some processes wish to enter, the selection of the next process cannot be postponed indefinitely. Only processes not in their remainder section can participate in the decision.

3. **Bounded Waiting**
   A limit must exist on the number of times other processes can enter their CS after a process requests entry and before that request is granted. Prevents starvation.

**Relationship:** All three are necessary. Mutual exclusion alone could cause deadlock. Progress without bounded waiting could cause starvation. All three together ensure correct, fair synchronization.

### Synchronization Mechanisms

#### 1. Mutex Locks

**Description:**
Simplest synchronization tool. A binary lock that protects a critical section.

**Operations:**
- `acquire()` - Obtain the lock before entering CS
- `release()` - Release the lock when exiting CS

**Assessment:**
- **Advantage:** Simple concept
- **Disadvantage:** Requires **busy waiting** (spinlock) - process loops waiting for lock, wasting CPU cycles
- Both operations must be **atomic** (indivisible)

#### 2. Semaphores

**Description:**
An integer variable S accessed only through two atomic operations:
- `wait(S)` or `P(S)` - Decrement; wait if value becomes negative
- `signal(S)` or `V(S)` - Increment; may wake a waiting process

**Types:**

**Binary Semaphore:**
- Range: 0 or 1
- Equivalent to a mutex lock
- Used for mutual exclusion

**Counting Semaphore:**
- Range: unrestricted integer domain
- Can control access to a resource with a finite number of instances
- Initial value = number of available resources

**Can counting semaphores control access to resources with finite instances?**
**Yes** - initialize the semaphore to the number of available resources. Each `wait()` consumes one resource; each `signal()` releases one.

**Implementation Without Busy Waiting:**
Use a **waiting queue** to store blocked processes. When a process must wait, it's placed in the queue and its state becomes waiting. When `signal()` is called, a process from the queue is awakened and moved to the ready queue.

**Semaphores vs. Mutex Locks:**
- Semaphores are more general (counting vs. binary)
- Semaphores can be used for synchronization beyond mutual exclusion
- Mutex is simpler, semantically clearer for mutual exclusion

#### 3. Monitors

**Description:**
A high-level abstraction (Abstract Data Type) providing a convenient and effective process synchronization mechanism. Only one process can be active within the monitor at any time.

**Condition Variables:**
Used to suspend and resume processes within the monitor:
- `x.wait()` - Suspend the calling process
- `x.signal()` - Resume one waiting process

**Assessment:**
- **Advantage:** Easier to ensure correctness than semaphores; mutual exclusion built-in
- **Disadvantage:** Not as widely supported; requires language/compiler support

### Standard Concurrency Problems

#### Bounded-Buffer (Producer-Consumer)

**Problem:**
Producer creates items and places them in a bounded buffer. Consumer removes items. Buffer has limited capacity.

**Solution Using Semaphores (Assignment 1 Concept):**

```
Semaphores:
- mutex = 1      // Mutual exclusion for buffer access
- empty = n      // Count of empty slots
- full = 0       // Count of full slots

Producer:
  wait(empty)    // Wait if buffer full
  wait(mutex)    // Enter critical section
  [add item to buffer]
  signal(mutex)  // Exit critical section
  signal(full)   // Signal item added

Consumer:
  wait(full)     // Wait if buffer empty
  wait(mutex)    // Enter critical section
  [remove item from buffer]
  signal(mutex)  // Exit critical section
  signal(empty)  // Signal slot freed
```

**Assignment 1 Specifics:**
- Uses shared memory instead of semaphores
- Must manage `in` and `out` pointers to track buffer positions
- Detect full: `(in + 1) % bufSize == out`
- Detect empty: `in == out`
- Use provided functions: `GetIn()`, `SetIn()`, `GetOut()`, `SetOut()`, `WriteAtBufIndex()`, `ReadAtBufIndex()`

**Structural Changes Analysis:**
If producer/consumer logic changes (e.g., different wait conditions, modified pointer management), analyze:
- What condition causes blocking?
- Does the new structure maintain mutual exclusion?
- Can deadlock or starvation occur?

#### Readers-Writers Problem

**First Readers-Writers Problem:**
No reader should wait unless a writer has already obtained permission. Readers have priority; writers may starve.

**Second Readers-Writers Problem:**
Once a writer is ready, it performs its write ASAP. Writers have priority; readers may starve.

**When is a Reader-Writer Lock Preferable to a Semaphore?**
When an application has many more reads than writes. Multiple readers can access data concurrently without conflicts, improving performance over exclusive semaphore access.

#### Dining Philosophers Problem

**Monitor Solution Starvation Scenario:**
A philosopher could starve if their neighbors continually acquire resources before them. For instance, if philosophers 0 and 2 keep eating, philosopher 1 may never get both chopsticks.

---

## 8. Deadlocks

### Deadlock Characterization

**What is Deadlock?**
A situation where a set of processes are blocked, each waiting for a resource held by another process in the set. No process can proceed.

### Four Necessary Conditions for Deadlock

Deadlock can occur only if all four conditions hold simultaneously:

1. **Mutual Exclusion**
   At least one resource must be nonsharable (only one process can use it at a time).

2. **Hold and Wait**
   A process must be holding at least one resource and waiting to acquire additional resources currently held by other processes.

3. **No Preemption**
   Resources cannot be forcibly taken away; they can only be released voluntarily by the process holding them after it completes its task.

4. **Circular Wait**
   A closed chain of processes exists, where each process holds at least one resource needed by the next process in the chain.

**Breaking any one condition prevents deadlock.**

### Deadlock Handling Methods

**Three General Approaches:**

1. **Prevention**
   Ensure at least one of the four necessary conditions cannot hold. Design system to make deadlock impossible.

2. **Avoidance**
   Dynamically examine resource allocation state; only grant requests that keep the system in a safe state. Requires advance knowledge of resource needs.

3. **Detection and Recovery / Ignoring**
   Allow deadlocks to occur, then detect and recover. Or ignore (ostrich algorithm) if deadlocks are rare.

### Deadlock Prevention Protocols

**Preventing Hold-and-Wait:**
- **Protocol 1:** Require processes to request and be allocated all resources before execution begins
- **Protocol 2:** Allow processes to request resources only when they have none (must release all current resources before requesting new ones)

**Assessment:** Both cause low resource utilization and potential starvation.

### Safe and Unsafe States

**Unsafe State:**
Does **not** necessarily lead to deadlock - it means deadlock is *possible*. An unsafe state may or may not result in deadlock depending on future resource requests.

**Safe State:**
The system can allocate resources to each process in some order and avoid deadlock. A safe sequence exists.

---

## 9. Memory Management (Real Memory)

### Address Binding

**When are symbolic addresses bound to physical addresses?**

**Three Binding Times:**

1. **Compile Time**
   If memory location is known at compile time, absolute code is generated. Must recompile if starting location changes.

2. **Load Time**
   If memory location is unknown at compile time, compiler generates relocatable code. Binding occurs when the program is loaded.

3. **Execution Time**
   Binding delayed until runtime if process can move during execution. Requires hardware support (base/limit registers). **Most general-purpose OSs use execution time binding** for flexibility.

### Memory Allocation Schemes

#### 1. Contiguous Allocation

**Description:**
Each process occupies a single contiguous section of memory.

**Hardware Support:**
- **Base register** - Smallest physical address accessible by process
- **Limit register** - Range of logical addresses
- CPU compares every address against these registers for protection

**Assessment:**
- **Advantage:** Simple, fast access
- **Problem:** **External fragmentation** - total free memory exists but is scattered in small holes

#### 2. Segmentation

**Description:**
Memory management scheme supporting the user's view of memory. A program is a collection of logical units (segments): main program, procedures, functions, stack, heap, etc.

**Logical Address:**
Two-dimensional: `<segment-number, offset>`

**Segment Table:**
Maps segment numbers to physical base addresses and limit (length).

**Assessment:**
- **Advantage:** Matches logical structure of programs; can share/protect segments independently
- **Problem:** Still suffers from external fragmentation (segments vary in size)

#### 3. Paging

**Description:**
Physical address space can be noncontiguous, eliminating external fragmentation. Physical memory divided into fixed-size **frames**; logical memory divided into same-size **pages**.

**Address Translation:**
Logical address: `<page number p, offset d>`
- **Page table** maps page number to frame number
- Physical address: `<frame number, offset>`

**Calculating Page Number:**
Given logical address and page size, page number = logical address / page size (integer division).

**Example:** Logical address 3085, page size 1024:
Page number = 3085 / 1024 = 3

**Translation Look-aside Buffer (TLB):**
A fast associative memory cache holding recent page table entries. Speeds up address translation since page tables are typically in main memory.

**How does TLB assist translation?**
TLB stores recent page-to-frame mappings. On a memory reference, the TLB is searched first (in parallel). If found (TLB hit), the frame number is immediately available. If not (TLB miss), the page table must be accessed in main memory.

**Assessment:**
- **Advantage:** No external fragmentation; easy to allocate memory
- **Problem:** **Internal fragmentation** - allocated memory may be slightly larger than requested (last page may not be full)

### Fragmentation

**External Fragmentation:**
Total free memory exists to satisfy a request, but it is scattered in non-contiguous small blocks. Occurs with variable-sized partitions (contiguous allocation, segmentation).

**Internal Fragmentation:**
Allocated memory is slightly larger than requested. The extra memory, internal to a partition, is not used. Occurs with fixed-sized partitions (paging).

---

## 10. Virtual Memory Management

### Virtual Memory Fundamentals

**What is Virtual Memory?**
A technique that separates user logical memory from physical memory. Only part of a program needs to be in memory for execution.

**Benefits:**
- Logical address space can be much larger than physical memory
- More programs can run concurrently
- Less I/O needed to load/swap programs
- Faster program loading and response time

### Demand Paging

**Description:**
A page is brought into memory only when it is needed (referenced during execution). The opposite is bringing the entire program into memory at load time.

**When is a page loaded?**
Only when needed during execution - specifically, when a page fault occurs referencing that page.

**Valid-Invalid Bit:**
Each page table entry has a bit:
- **Valid (v)** - Page is in memory (legal and in physical memory)
- **Invalid (i)** - Page is not in memory (either illegal or legal but not currently loaded)

**Page Fault Sequence:**
1. Reference to invalid page causes a trap to OS (page fault)
2. OS checks internal table: invalid reference (abort) or page not in memory?
3. If valid reference, find a free frame
4. Schedule disk operation to read page into frame
5. Update page table (set bit to valid)
6. Restart instruction that caused the page fault

**Assessment:**
- **Advantage:** Reduced memory requirements, faster program startup, more concurrent processes
- **Disadvantage:** Requires hardware support (page table with valid bit); slow page fault service time

### Effective Access Time (EAT)

Performance of demand paging depends critically on the **page fault rate (p)**.

**Formula:**
```
EAT = (1 - p) \times memory_access_time + p \times page_fault_service_time
```

**Example:**
- Memory access: 200 ns
- Page fault service (including disk I/O): 8 ms = 8,000,000 ns
- If p = 0.001 (one fault per 1000 accesses):

```
EAT = 0.999 \times 200 + 0.001 \times 8,000,000
    = 199.8 + 8,000
    = 8,199.8 ns
```

Performance is 40\times slower than memory access! The page fault rate must be very small for acceptable performance.

### Page Replacement

**When is it needed?**
When a page fault occurs and no free frames are available in physical memory.

**Modify (Dirty) Bit:**
Indicates whether a page has been modified since loading.
- **0 (clean)** - Page unchanged; no need to write back to disk (discard)
- **1 (dirty)** - Page modified; must write back to disk before replacing

**What does the dirty bit signal?**
The page has been modified since it was loaded into memory and must be saved back to disk if selected as a victim.

### Page Replacement Algorithms

#### 1. FIFO (First-In-First-Out)

**Description:**
Replace the page that has been in memory the longest (oldest).

**Assessment:**
- **Advantage:** Simple to implement
- **Problem:** **Belady's Anomaly** - increasing the number of frames can sometimes increase the page fault rate
- Poor performance; may replace frequently used pages

#### 2. Optimal (OPT)

**Description:**
Replace the page that will not be used for the longest period of time in the future.

**Assessment:**
- **Advantage:** Lowest possible page fault rate
- **Disadvantage:** Impossible to implement (requires predicting future)
- Used only as a theoretical benchmark for comparison

#### 3. Least Recently Used (LRU)

**Description:**
Replace the page that has not been used for the longest time in the past. Uses past behavior to predict future behavior.

**Assessment:**
- **Advantage:** Generally excellent performance; does not suffer from Belady's Anomaly
- **Disadvantage:** Requires expensive hardware support to track page usage times (counters or stack for every memory reference)
- Considered one of the best practical algorithms

#### 4. LRU Approximation (Second-Chance, Clock)

**Description:**
Uses a **reference bit** for each page. When a page is referenced, its bit is set to 1. For replacement:
- Examine pages in FIFO order
- If reference bit = 0, replace this page
- If reference bit = 1, clear it to 0 and check next oldest page
- Give each page a "second chance"

**Assessment:**
- **Advantage:** Efficient approximation of LRU without expensive hardware
- **Disadvantage:** Not as effective as true LRU
- Widely used in practice

### Demand Paging vs. Paging with Swapping

**Demand Paging:**
Pages are loaded from disk only when referenced (lazy loading). Pages may never be swapped back to disk unless the frame is needed.

**Paging with Swapping:**
Entire processes can be swapped out to disk to free memory. When swapped back in, all pages return to memory. More aggressive memory management.

**Key Distinction:** Demand paging is page-level and on-demand; swapping is process-level and typically involves all pages.

### Thrashing

**What is Thrashing?**
A state where a process is spending more time paging (swapping pages in and out) than executing. Occurs when a process doesn't have enough frames to hold its **working set** (the set of pages actively used).

**Symptoms:**
- Low CPU utilization (processes always waiting for page I/O)
- High paging activity
- Severe performance degradation

**Why doesn't local replacement entirely solve thrashing?**
Local replacement (replacing only the process's own pages) prevents one process from stealing frames from others, but doesn't solve the fundamental problem: if a process doesn't have enough frames for its working set, it will thrash regardless of the replacement policy. The only solution is to allocate more frames or reduce the degree of multiprogramming.

**Prevention:**
- Use working set model to allocate sufficient frames
- Use local or priority replacement policies
- Suspend processes if insufficient memory

---

## 11. Storage Systems and File Systems

### Disk Scheduling

#### Positioning Time Components

**Two Components:**
1. **Seek Time** - Time to move the disk arm to the desired cylinder
2. **Rotational Latency** - Time for the desired sector to rotate under the read/write head

**Total positioning time = seek time + rotational latency**

#### Disk Scheduling Algorithms

**FCFS (First-Come, First-Served):**
- **Description:** Services requests in arrival order
- **Characteristic:** Ignores current head position
- **Disadvantage:** Can result in long waits and inefficient head movement (wild swings across disk)

**Other Algorithms (SSTF, SCAN, C-SCAN, LOOK, C-LOOK):**
Consider head position to minimize seek time and improve throughput.

#### Factors Influencing Algorithm Selection

- Request arrival patterns
- File allocation methods
- Disk load
- Required response time characteristics
- Fairness vs. efficiency trade-offs

### File System Implementation

#### Structures

**On-Disk Structures:**
- **Boot Control Block** - Information to boot OS from this volume (boot block in UNIX, partition boot sector in Windows)
- **Volume Control Block** - Volume/partition details (superblock in UNIX, master file table in NTFS)
- **Directory Structure** - Organizes files
- **Per-File File Control Block (FCB)** - File details (inode in UNIX)

**Which structure implements a file system?**
All of the above work together. The boot control block, volume control block, and directory structure are the primary organizational structures.

**Which structure reads/writes physical blocks?**
The **basic file system** (I/O control layer) issues generic commands to device drivers and reads/writes physical blocks.

#### Allocation Methods

**Linked Allocation:**
Each file is a linked list of disk blocks scattered anywhere on the disk.

**Why is the entire block unavailable to the user?**
Part of each block must be reserved for the pointer to the next block in the chain. If blocks are 512 bytes, and pointers are 4 bytes, only 508 bytes are available for user data per block.

---

## 12. OS Design and Implementation

### Design Principles

#### Policy vs. Mechanism

**Critical Design Principle:**
Separate **policy** (what will be done?) from **mechanism** (how to do it?).

**Mechanism:**
Determines how to accomplish a task (the implementation details).

**Policy:**
Decides what needs to be done (the decision-making rules).

**Why separate them?**
Flexibility - policy changes don't require changing underlying mechanisms. Allows the same OS to adapt to different environments by changing policies without rewriting code.

**Example:**
- **Mechanism:** Timer interrupt hardware and context switching code
- **Policy:** CPU scheduling algorithm (RR, SJF, Priority)

### OS Structures

#### 1. Simple Structure (MS-DOS)

**Characteristics:**
- Not divided into modules
- Interfaces and functionality not well separated
- Application programs can access basic I/O routines and hardware directly

**Assessment:**
- Simple to implement initially
- Poor protection and reliability
- Difficult to maintain and extend

#### 2. Traditional UNIX

**Characteristics:**
- Limited structuring
- Two layers: system programs and the kernel
- The kernel is everything below the system-call interface and above hardware
- Monolithic kernel with all services in kernel space

**Assessment:**
- Efficient (everything in kernel, no message passing overhead)
- Difficult to maintain and extend
- Poor isolation between kernel components

#### 3. Microkernel

**Characteristics:**
- Moves as much functionality as possible out of kernel into user space
- Minimal kernel: IPC, basic scheduling, low-level memory management
- Services communicate via **message passing**

**How do microkernels handle communication?**
Through **message passing** - services send messages to each other through the kernel's IPC mechanism.

**Assessment:**
- **Advantages:**
  - Easier to extend (add services in user space)
  - Easier to port to new hardware
  - More reliable (service failures don't crash kernel)
  - More secure (less code in privileged mode)
- **Disadvantages:**
  - Performance overhead due to message passing and user/kernel space transitions

#### 4. Hybrid Systems

**Characteristics:**
Most modern OSs combine multiple approaches for practical reasons.

**Examples:**
- **Linux/Solaris:** Monolithic kernel with loadable modules
- **Mac OS X:** Mach microkernel foundation + BSD UNIX components + other layers
- **Windows:** Modular monolithic kernel with some microkernel concepts

---

## 13. Assignment 1 - Shared Memory Producer-Consumer

### Overview

Implement producer and consumer programs that communicate through a **bounded buffer in shared memory**.

### Key Components

#### Shared Memory Structure (4KB block)

**Header:**
- `bufSize` - Buffer capacity (number of items)
- `itemCnt` - Total items to produce/consume
- `in` - Index where next item will be produced
- `out` - Index where next item will be consumed

**Buffer Area:**
Contains the actual data items following the header.

### Process Creation

1. **Producer (parent)** creates shared memory using `InitShm()`
2. Producer uses `fork()` to create **Consumer (child)**
3. Producer uses `exec()` to load consumer executable
4. Child inherits access to shared memory

### Synchronization Requirements

**Buffer Full Condition:**
```
(in + 1) % bufSize == out
```
Producer must **wait** if buffer is full before producing.

**Buffer Empty Condition:**
```
in == out
```
Consumer must **wait** if buffer is empty before consuming.

### Required Functions

**Pointer Management:**
- `GetIn()` - Read current `in` value
- `SetIn(new_value)` - Update `in` value
- `GetOut()` - Read current `out` value
- `SetOut(new_value)` - Update `out` value

**Buffer Access:**
- `WriteAtBufIndex(index, data)` - Write data at buffer index
- `ReadAtBufIndex(index)` - Read data from buffer index

**Critical:** Use these functions to maintain synchronization and avoid race conditions.

### Implementation Tips

1. Always check buffer full/empty before producing/consuming
2. Use modulo arithmetic for circular buffer indices
3. Produce/consume one item at a time
4. Update `in`/`out` pointers after each operation
5. Handle the case where `itemCnt` items have been processed

---

## 14. Assignment 2 - CPU Scheduling Simulation

### Overview

Simulate four CPU scheduling algorithms on a single CPU system, reading process data from input and producing scheduling statistics.

### Input Format

Each process has:
- Arrival time
- CPU burst length
- Priority (for priority scheduling)
- Process ID

### Algorithms to Implement

#### 1. Round Robin (RR)

**Parameters:** Time quantum (given)

**Algorithm:**
1. Maintain FIFO ready queue
2. Add arriving processes to queue in arrival order
3. Execute front process for quantum or until completion
4. If quantum expires, preempt and add to end of queue
5. Repeat until all processes complete

**Special Cases:**
- Process completes before quantum expires
- Multiple processes arrive at same time (add in input order)

#### 2. Shortest Job First (SJF) - Non-preemptive

**Algorithm:**
1. Scheduling decisions only when current process terminates
2. Select process from ready queue with shortest CPU burst
3. Ties broken by arrival time (FCFS)
4. Execute selected process to completion

**Key:** Once a process starts, it runs to completion (non-preemptive).

#### 3. Priority Scheduling - No Preemption

**Algorithm:**
1. Scheduling decisions only when current process terminates
2. Select process from ready queue with highest priority (smallest number)
3. Ties broken arbitrarily
4. Execute selected process to completion

#### 4. Priority Scheduling - With Preemption

**Algorithm:**
1. Scheduling decisions on termination OR when higher priority arrives
2. When new process arrives:
   - If its priority > current process priority, preempt current process
   - Add preempted process back to ready queue
3. Select highest priority ready process
4. Execute until completion or preemption

**Key Difference:** Can interrupt running process if higher priority arrives.

### Output Requirements

For each algorithm, calculate and report:
- **Completion time** for each process
- **Turnaround time** (completion - arrival)
- **Waiting time** (turnaround - burst)
- **Average turnaround time**
- **Average waiting time**
- **Gantt chart** showing process execution timeline

### Implementation Considerations

1. **Event-driven simulation** - Process events in time order (arrivals, completions, quantum expirations)
2. **Ready queue management** - Different data structures for different algorithms (FIFO queue, priority queue, sorted list)
3. **Context switching** - Track when processes start/stop
4. **Tie-breaking** - Be consistent with specified rules
5. **Edge cases** - Multiple simultaneous events, idle CPU time

---

## 15. Quick Reference Tables

### Process States Summary

| Transition | Trigger |
|------------|---------|
| New → Ready | Process admitted |
| Ready → Running | Scheduler selects process |
| Running → Waiting | I/O request or event wait |
| Running → Ready | Interrupt (timer, preemption) |
| Waiting → Ready | I/O or event completion |
| Running → Terminated | Process exits |

### Scheduling Algorithm Selection Guide

| Algorithm | Best For | Worst For |
|-----------|----------|-----------|
| FCFS | Simple batch systems | Interactive systems (convoy effect) |
| SJF/SRTF | Minimizing average waiting time | Long processes (starvation risk) |
| Priority | Important tasks need preference | Ensuring fairness (starvation risk) |
| RR | Interactive systems, time-sharing | Minimizing turnaround time |
| Multilevel Feedback | Mixed workloads (interactive + CPU-bound) | Simplicity of implementation |

### Synchronization Tool Comparison

| Tool | Complexity | Requires Hardware | Busy Waiting | Best Use |
|------|------------|-------------------|--------------|----------|
| Mutex Lock | Low | Yes (atomic ops) | Yes | Simple mutual exclusion |
| Binary Semaphore | Medium | Yes (atomic ops) | Optional | Mutual exclusion |
| Counting Semaphore | Medium | Yes (atomic ops) | Optional | Resource counting |
| Monitor | High | No (language support) | No | Complex synchronization |

### Memory Management Comparison

| Scheme | Fragmentation | Address Translation | Flexibility |
|--------|---------------|---------------------|-------------|
| Contiguous | External | Base/Limit registers | Low |
| Segmentation | External | Segment table | Medium |
| Paging | Internal | Page table | High |
| Virtual Memory | Internal | Page table + disk | Highest |

---

## 16. Common Exam Question Patterns

### Calculation Problems

**1. Page Number from Logical Address:**
- Page number = logical_address / page_size (integer division)
- Offset = logical_address % page_size

**2. Context Switch Time with Swapping:**
```
Total time = 2 \times (swap_out_time + swap_in_time) + actual_context_switch
Swap time = (process_size / transfer_rate) + latency
```

**3. Effective Access Time:**
```
EAT = (1 - p) \times mem_access + p \times page_fault_time
```

### Conceptual Questions

**Operating System as Government:**
Creates environment for programs to work; manages resources; doesn't do productive work itself.

**Items Shared by Threads:**
Code, data, files. Each thread has its own: PC, registers, stack.

**Why Google Chrome Uses Multiple Processes:**
Isolation for stability and security; tab/plugin crashes don't affect others.

**Aging Purpose:**
Prevents starvation by gradually increasing priority of waiting processes.

**Thrashing:**
Excessive paging; process doesn't have enough frames for working set; CPU utilization drops.

---

## Key Takeaways for Assignments

### Assignment 1 Success Factors

[CHECK] Understand circular buffer logic
[CHECK] Check full/empty conditions correctly
[CHECK] Use provided functions exclusively
[CHECK] Handle synchronization with shared variables
[CHECK] Test with various buffer sizes and item counts

### Assignment 2 Success Factors

[CHECK] Implement event-driven simulation
[CHECK] Use appropriate data structures for each algorithm
[CHECK] Handle tie-breaking consistently
[CHECK] Track all timing information accurately
[CHECK] Test with edge cases (simultaneous arrivals, process completion timing)
[CHECK] Verify calculations match algorithm specifications

---

*End of Study Guide*